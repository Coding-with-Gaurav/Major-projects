{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'license': 'MIT License\\n\\nCopyright (c) [year] [fullname]\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.',\n",
       " 'modelfile': '# Modelfile generated by \"ollama show\"\\n# To build a new Modelfile based on this, replace FROM with:\\n# FROM zephyr:latest\\n\\nFROM C:\\\\Users\\\\23146007\\\\.ollama\\\\models\\\\blobs\\\\sha256-730ebed2578e5be3e25c3ba155b06cb46690426682a38127cb72a9697e4443b4\\nTEMPLATE \"{{- if .System }}\\n<|system|>\\n{{ .System }}\\n</s>\\n{{- end }}\\n<|user|>\\n{{ .Prompt }}\\n</s>\\n<|assistant|>\\n\"\\nPARAMETER stop <|system|>\\nPARAMETER stop <|user|>\\nPARAMETER stop <|assistant|>\\nPARAMETER stop </s>\\nLICENSE \"\"\"MIT License\\n\\nCopyright (c) [year] [fullname]\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\"\"\"\\n',\n",
       " 'parameters': 'stop                           \"<|system|>\"\\nstop                           \"<|user|>\"\\nstop                           \"<|assistant|>\"\\nstop                           \"</s>\"',\n",
       " 'template': '{{- if .System }}\\n<|system|>\\n{{ .System }}\\n</s>\\n{{- end }}\\n<|user|>\\n{{ .Prompt }}\\n</s>\\n<|assistant|>\\n',\n",
       " 'details': {'parent_model': '',\n",
       "  'format': 'gguf',\n",
       "  'family': 'llama',\n",
       "  'families': ['llama'],\n",
       "  'parameter_size': '7B',\n",
       "  'quantization_level': 'Q4_0'},\n",
       " 'model_info': {'general.architecture': 'llama',\n",
       "  'general.file_type': 2,\n",
       "  'general.parameter_count': 7241732096,\n",
       "  'general.quantization_version': 2,\n",
       "  'llama.attention.head_count': 32,\n",
       "  'llama.attention.head_count_kv': 8,\n",
       "  'llama.attention.layer_norm_rms_epsilon': 1e-05,\n",
       "  'llama.block_count': 32,\n",
       "  'llama.context_length': 32768,\n",
       "  'llama.embedding_length': 4096,\n",
       "  'llama.feed_forward_length': 14336,\n",
       "  'llama.rope.dimension_count': 128,\n",
       "  'llama.rope.freq_base': 10000,\n",
       "  'tokenizer.ggml.bos_token_id': 1,\n",
       "  'tokenizer.ggml.eos_token_id': 2,\n",
       "  'tokenizer.ggml.merges': None,\n",
       "  'tokenizer.ggml.model': 'llama',\n",
       "  'tokenizer.ggml.padding_token_id': 2,\n",
       "  'tokenizer.ggml.scores': None,\n",
       "  'tokenizer.ggml.token_type': None,\n",
       "  'tokenizer.ggml.tokens': None,\n",
       "  'tokenizer.ggml.unknown_token_id': 0},\n",
       " 'modified_at': '2024-10-11T14:41:43.8940868+05:30'}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import json\n",
    "import ollama.client as client\n",
    "\n",
    "model = 'zephyr'\n",
    "\n",
    "# response, _ = client.generate(model_name=model,prompt='prompt' )\n",
    "client.show('zephyr:latest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graphPrompt(input: str, metadata={}, model=\"mistral-openorca:latest\"):\n",
    "    if model == None:\n",
    "        model = \"mistral-openorca:latest\"\n",
    "\n",
    "    # model_info = client.show(model_name=model)\n",
    "    # print( chalk.blue(model_info))\n",
    "\n",
    "    SYS_PROMPT = (\n",
    "        \"You are a network graph maker who extracts terms and their relations from a given context. \"\n",
    "        \"You are provided with a context chunk (delimited by ```) Your task is to extract the ontology \"\n",
    "        \"of terms mentioned in the given context. These terms should represent the key concepts as per the context. \\n\"\n",
    "        \"Thought 1: While traversing through each sentence, Think about the key terms mentioned in it.\\n\"\n",
    "            \"\\tTerms may include object, entity, location, organization, person, \\n\"\n",
    "            \"\\tcondition, acronym, documents, service, concept, etc.\\n\"\n",
    "            \"\\tTerms should be as atomistic as possible\\n\\n\"\n",
    "        \"Thought 2: Think about how these terms can have one on one relation with other terms.\\n\"\n",
    "            \"\\tTerms that are mentioned in the same sentence or the same paragraph are typically related to each other.\\n\"\n",
    "            \"\\tTerms can be related to many other terms\\n\\n\"\n",
    "        \"Thought 3: Find out the relation between each such related pair of terms. \\n\\n\"\n",
    "        \"Format your output as a list of json. Each element of the list contains a pair of terms\"\n",
    "        \"and the relation between them, like the follwing: \\n\"\n",
    "        \"[\\n\"\n",
    "        \"   {\\n\"\n",
    "        '       \"node_1\": \"A concept from extracted ontology\",\\n'\n",
    "        '       \"node_2\": \"A related concept from extracted ontology\",\\n'\n",
    "        '       \"edge\": \"relationship between the two concepts, node_1 and node_2 in one or two sentences\"\\n'\n",
    "        \"   }, {...}\\n\"\n",
    "        \"]\"\n",
    "    )\n",
    "\n",
    "    USER_PROMPT = f\"context: ```{input}``` \\n\\n output: \"\n",
    "    response, _ = client.generate(\n",
    "        model_name=model, system=SYS_PROMPT, prompt=USER_PROMPT\n",
    "    )\n",
    "    try:\n",
    "        result = json.loads(response)\n",
    "        result = [dict(item, **metadata) for item in result]\n",
    "    except:\n",
    "        print(\"\\n\\nERROR ### Here is the buggy response: \", response, \"\\n\\n\")\n",
    "        result = None\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "   {\n",
      "       \"node_1\": \"Mary\",\n",
      "       \"node_2\": \"lamb\",\n",
      "       \"edge\": \"Mary had a little lamb\"\n",
      "   },\n",
      "   {\n",
      "       \"node_1\": \"Mary\",\n",
      "       \"node_2\": \"plate\",\n",
      "       \"edge\": \"Mary passed her plate\"\n",
      "   },\n",
      "   {\n",
      "       \"node_1\": \"Mary\",\n",
      "       \"node_2\": \"more\",\n",
      "       \"edge\": \"And had a little more\"\n",
      "   }\n",
      "]\n",
      "```\n",
      "\n",
      "ERROR ### Here is the buggy response:  [\n",
      "   {\n",
      "       \"node_1\": \"Mary\",\n",
      "       \"node_2\": \"lamb\",\n",
      "       \"edge\": \"Mary had a little lamb\"\n",
      "   },\n",
      "   {\n",
      "       \"node_1\": \"Mary\",\n",
      "       \"node_2\": \"plate\",\n",
      "       \"edge\": \"Mary passed her plate\"\n",
      "   },\n",
      "   {\n",
      "       \"node_1\": \"Mary\",\n",
      "       \"node_2\": \"more\",\n",
      "       \"edge\": \"And had a little more\"\n",
      "   }\n",
      "]\n",
      "``` \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "r = graphPrompt(\n",
    "    \"\"\"\n",
    "text delimited bweteen ``` \\n\n",
    "text: ```\n",
    "Mary had a little lamb,\n",
    "You've heard this tale before;\n",
    "But did you know she passed her plate, \n",
    "And had a little more!\n",
    "```\n",
    "\"\"\", model='zephyr:latest'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "OpenAI@3111",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
